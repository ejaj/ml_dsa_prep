{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How do you handle missing or corrupted data in a dataset?***\n",
    "\n",
    "Techniques:\n",
    "\n",
    "    1. Drop missing values\n",
    "    2. Impute with mean/median/mode\n",
    "    3. Predict missing values using a model\n",
    "    4. Use domain-specific constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age gender\n",
      "0  25.0      M\n",
      "1  25.0      F\n",
      "2  30.0   None\n",
      "3  22.0      M\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, np.nan, 30, 22],\n",
    "    'gender': ['M', 'F', None, 'M']\n",
    "})\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Impute nuerical with median\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "\n",
    "# Impute categorical with mode\n",
    "df['gender'] = df['gender'].fillna(df['gender'].mode()[0])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': [25, np.nan, 30, 22],\n",
    "    'gender': ['M', 'F', None, 'M']\n",
    "})\n",
    "\n",
    "num_cols = ['age']\n",
    "cat_cols = ['gender']\n",
    "\n",
    "# Impute numerical with median\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# Impute categorical with most frequent (mode)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How would you handle an imbalanced dataset?***\n",
    "\n",
    "Techniques:\n",
    "\n",
    "1. Resampling (undersample majority / oversample minority)\n",
    "2. SMOTE (Synthetic Minority Oversampling)\n",
    "3. Class weights\n",
    "3. Use appropriate metrics (AUC, F1, Precision-Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    90\n",
      "1    90\n",
      "Name: count, dtype: int64\n",
      "   feature1  feature2  label\n",
      "0 -1.701414  0.273864      0\n",
      "1 -0.404602  1.084066      0\n",
      "2  1.056552  0.290671      1\n",
      "3  0.806050 -1.999391      0\n",
      "4  0.978726  0.673321      1\n",
      "Original class distribution: Counter({0: 90, 1: 10})\n",
      "After SMOTE class distribution: Counter({0: 90, 1: 90})\n",
      "   feature1  feature2  label\n",
      "0 -0.177564  1.574800      0\n",
      "1  0.021793 -1.695312      0\n",
      "2 -0.313299  3.204328      0\n",
      "3 -1.648307 -0.395977      0\n",
      "4  0.746839  1.502410      0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(100),\n",
    "    'feature2': np.random.randn(100),\n",
    "    'label': [0]*90 + [1]*10   # Imbalanced: 90 class 0, 10 class 1\n",
    "})\n",
    "majority = df[df.label == 0]\n",
    "minority = df[df.label == 1]\n",
    "\n",
    "minority_upsampled = resample(\n",
    "    minority,\n",
    "    replace=True,\n",
    "    n_samples=len(majority),\n",
    "    random_state=42\n",
    ")\n",
    "df_balanced = pd.concat([majority, minority_upsampled])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Check new class distribution\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# Optional: display a few rows\n",
    "print(df_balanced.head())\n",
    "\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['label']\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X,y)\n",
    "\n",
    "df_balanced = pd.DataFrame(X_resampled, columns=['feature1', 'feature2'])\n",
    "df_balanced['label'] = y_resampled\n",
    "\n",
    "# Step 6: Check new distribution\n",
    "print(\"After SMOTE class distribution:\", Counter(y_resampled))\n",
    "\n",
    "# Optional: preview the balanced dataset\n",
    "print(df_balanced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How do you handle categorical variables in your dataset?***\n",
    "\n",
    "Techniques:\n",
    "\n",
    "1. One-hot encoding\n",
    "2. Label encoding\n",
    "3. Binary encoding\n",
    "4. Embeddings (for high-cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0547, -0.4482],\n",
      "        [ 0.9830, -0.6250],\n",
      "        [ 1.8742,  0.2865],\n",
      "        [ 0.9830, -0.6250],\n",
      "        [ 1.0547, -0.4482]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'color': ['red', 'blue', 'green']})\n",
    "\n",
    "# One-hot encoding\n",
    "pd.get_dummies(df, columns=['color'])\n",
    "\n",
    "# Label conding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['color_encoded'] = le.fit_transform(df['color'])\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#  3 categories → embedding dim 2\n",
    "embedding = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "\n",
    "# Assume 'red'=2, 'blue'=0, 'green'=1 as per LabelEncoder\n",
    "category_ids = torch.LongTensor([2, 0, 1, 0, 2])\n",
    "embedded = embedding(category_ids)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***How do filtering and wrapper methods work in feature selection?***\n",
    " 1. Filter Method\n",
    " 2. Wrapper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (Filter method):\n",
      "Index(['mean perimeter', 'mean concave points', 'worst radius',\n",
      "       'worst perimeter', 'worst concave points'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X,y)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected features (Filter method):\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper Method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=5)\n",
    "X_rfe = rfe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (Wrapper method):\n",
      "Index(['mean concave points', 'worst radius', 'worst perimeter', 'worst area',\n",
      "       'worst concave points'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "selected_rfe = X.columns[rfe.support_]\n",
    "print(\"\\nSelected features (Wrapper method):\")\n",
    "print(selected_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How do you handle time-based features in a machine learning model?***\n",
    "\n",
    "Techniques:\n",
    "\n",
    "1. Extract year, month, day, hour, weekday\n",
    "2. Time differences (e.g., time since last event)\n",
    "3. Use rolling averages or lag features\n",
    "4. Encode cyclical features (e.g., hour of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id     event           timestamp\n",
      "0        1     login 2024-07-20 08:15:27\n",
      "1        2    logout 2024-07-20 09:45:00\n",
      "2        1  purchase 2024-07-21 12:30:15\n",
      "3        3     login 2024-07-22 23:10:10\n",
      "4        2  purchase 2024-07-23 00:05:05\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'user_id': [1, 2, 1, 3, 2],\n",
    "    'event': ['login', 'logout', 'purchase', 'login', 'purchase'],\n",
    "    'timestamp': [\n",
    "        '2024-07-20 08:15:27',\n",
    "        '2024-07-20 09:45:00',\n",
    "        '2024-07-21 12:30:15',\n",
    "        '2024-07-22 23:10:10',\n",
    "        '2024-07-23 00:05:05'\n",
    "    ]\n",
    "})\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_sin'] = np.sin(2*np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2*np.pi * df['hour'] / 24)\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['user_id', 'timestamp'])\n",
    "df['time_since_last_event'] = df.groupby('user_id')['timestamp'].diff().dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is feature hashing? When would you use it?***\n",
    "\n",
    "Feature Hashing:\n",
    "Maps categorical variables to a fixed-length hash space — useful for high-cardinality features.\n",
    "\n",
    "When:\n",
    "1. Text classification\n",
    "2. Large-scale categorical data (e.g., user IDs, URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 2 stored elements and shape (2, 10)>\n",
      "  Coords\tValues\n",
      "  (0, 5)\t-1.0\n",
      "  (1, 5)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "h = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = h.transform([{'user': '123'}, {'user': '456'}])\n",
    "print(hashed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How do you handle hierarchical categorical variables?***\n",
    "\n",
    "Strategies:\n",
    "\n",
    "1. Encode each level separately\n",
    "2. Combine levels: \"Country|City\"\n",
    "3. Use embeddings for each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country_DE  Country_US  State_BE  State_BW  State_CA  State_NY  \\\n",
      "0       False        True     False     False      True     False   \n",
      "1       False        True     False     False     False      True   \n",
      "2        True       False      True     False     False     False   \n",
      "3        True       False     False      True     False     False   \n",
      "\n",
      "   City_Berlin  City_LA  City_NYC  City_Stuttgart  \n",
      "0        False     True     False           False  \n",
      "1        False    False      True           False  \n",
      "2         True    False     False           False  \n",
      "3        False    False     False            True  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df = pd.DataFrame({\n",
    "    'Country': ['US', 'US', 'DE', 'DE'],\n",
    "    'State': ['CA', 'NY', 'BE', 'BW'],\n",
    "    'City': ['LA', 'NYC', 'Berlin', 'Stuttgart']\n",
    "})\n",
    "df_encoded = pd.get_dummies(df, columns=['Country', 'State', 'City'])\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country_City_DE|Berlin  Country_City_DE|Stuttgart  Country_City_US|LA  \\\n",
      "0                   False                      False                True   \n",
      "1                   False                      False               False   \n",
      "2                    True                      False               False   \n",
      "3                   False                       True               False   \n",
      "\n",
      "   Country_City_US|NYC  \n",
      "0                False  \n",
      "1                 True  \n",
      "2                False  \n",
      "3                False  \n"
     ]
    }
   ],
   "source": [
    "df['Country_City'] = df['Country'] + '|' + df['City']\n",
    "df_encoded = pd.get_dummies(df[['Country_City']])\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = pd.Series([1, 0, 1, 0])\n",
    "# df['city_target_mean'] = df['City'].map(df.groupby('City')[target.name].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Suppose: Country=3 unique, State=10, City=50\n",
    "country_emb = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "state_emb = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
    "city_emb = nn.Embedding(num_embeddings=50, embedding_dim=8)\n",
    "\n",
    "# During training, you pass ID tensors through these layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explain strategies for handling outliers in different ML algorithms***\n",
    "\n",
    "General Strategies:\n",
    "\n",
    "1. Remove outliers using Z-score or IQR\n",
    "2. Cap/floor values (Winsorization)\n",
    "3. Use robust models (e.g., tree-based)\n",
    "4. Transform data (log, sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Z-score filtering:\n",
      "   amount\n",
      "0      10\n",
      "1      12\n",
      "2      11\n",
      "3      13\n",
      "4      12\n",
      "5      11\n",
      "6    1000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "df = pd.DataFrame({\n",
    "    'amount': [10, 12, 11, 13, 12, 11, 1000]  # 1000 is an outlier\n",
    "})\n",
    "z_scores = np.abs(zscore(df['amount']))\n",
    "# Keep only data where z < 3\n",
    "df_z_filtered = df[z_scores < 3]\n",
    "\n",
    "print(\"After Z-score filtering:\")\n",
    "print(df_z_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After IQR filtering:\n",
      "   amount\n",
      "0      10\n",
      "1      12\n",
      "2      11\n",
      "3      13\n",
      "4      12\n",
      "5      11\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['amount'].quantile(0.25)\n",
    "Q3 = df['amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter\n",
    "df_iqr_filtered = df[(df['amount'] >= lower) & (df['amount'] <= upper)]\n",
    "\n",
    "print(\"\\nAfter IQR filtering:\")\n",
    "print(df_iqr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Winsorization:\n",
      "0      10\n",
      "1      12\n",
      "2      11\n",
      "3      13\n",
      "4      12\n",
      "5      11\n",
      "6    1000\n",
      "Name: amount_winsor, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Cap lowest and highest 10% values\n",
    "df['amount_winsor'] = winsorize(df['amount'], limits=[0.1, 0.1])\n",
    "\n",
    "print(\"\\nAfter Winsorization:\")\n",
    "print(df['amount_winsor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Log Transformation:\n",
      "0    2.397895\n",
      "1    2.564949\n",
      "2    2.484907\n",
      "3    2.639057\n",
      "4    2.564949\n",
      "5    2.484907\n",
      "6    6.908755\n",
      "Name: amount_log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['amount_log'] = np.log1p(df['amount'])  # log(1 + x) to handle 0s\n",
    "\n",
    "print(\"\\nAfter Log Transformation:\")\n",
    "print(df['amount_log'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
